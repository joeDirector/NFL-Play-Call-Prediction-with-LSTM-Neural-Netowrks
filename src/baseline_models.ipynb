{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Hyperparameters of Baseline Models (Logistic Regression, GBDT) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine optimal hyperparameters for predicting NFL play calls using Logistic Regression and Gradient Boosted Decision Trees. Uses a hyperband search strategy to tune various parameters for each model type. Data is kept non sequential. For comparing performance with sequential neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No supported GPU was found.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions For Model Specs and Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specify(specs):\n",
    "    \"\"\"\n",
    "    Make specifications for running the model tuning script.\n",
    "    Sets directories for storing results and global variables.\n",
    "    ---------------------------------------------\n",
    "    Inputs: A .json file containing user specifications\n",
    "    Returns: Various directories, a list of continous features, max play lags,\n",
    "    and distribution strategy\n",
    "    \"\"\"\n",
    "    # determine whether to run locally or on hpc\n",
    "    HPC = specs['HPC']['value']\n",
    "\n",
    "    # based on hpc decision, set directories for data and storing results\n",
    "    # also decide distribution strategy for keras tuner parallelization \n",
    "    if HPC:\n",
    "        # hpc data dir\n",
    "        data_dir = os.getcwd() + '/processed_pbp.csv'\n",
    "\n",
    "        # store results \n",
    "        results_dir = os.getcwd() + '/search_results'\n",
    "    \n",
    "    else:\n",
    "        # local data dir\n",
    "        data_dir = specs['LOCAL_DATA_DIR']['value']\n",
    "        \n",
    "        # local results dir\n",
    "        results_dir = specs['LOCAL_RESULTS_DIR']['value']\n",
    "\n",
    "    # get a list of continous feature variables\n",
    "    cont_feats = specs['CONT_FEATS']['value']\n",
    "\n",
    "    # get max play lags\n",
    "    max_lag = specs['MAX_PLAY_LAG']['value']\n",
    "\n",
    "    return data_dir, results_dir, cont_feats, max_lag\n",
    "# ********************************************************************\n",
    "def add_lagged_play_calls(input_df, max_lag):\n",
    "    \"\"\"\n",
    "    Given a pbp data frame, add feature columns\n",
    "    for up to max_lag lagged play call values. The \n",
    "    first max_lag play calls will be dropped for each \n",
    "    team.\n",
    "    ---------------------------------------------\n",
    "    Inputs: input_df: Pandas df shape (total_plays, n_features)\n",
    "            max_lag: (int) The maximum number of lagged play call\n",
    "            values to add as columns to the pbp data frame\n",
    "    Returns: A pbp data frame \n",
    "    \"\"\"\n",
    "    for lag in range(max_lag):\n",
    "        input_df['play_lag' + str(lag + 1)] = input_df.groupby(['posteam'])['pass'].shift(lag + 1)\n",
    "        \n",
    "    return input_df.dropna()\n",
    "# ********************************************************************\n",
    "def process_data(input_df, continous_feats, num_feats, prior_selected_feats = None, feat_select = False):\n",
    "    \"\"\"\n",
    "    Converts a pbp data frame into X and y matrices \n",
    "    for training/validation. Continous features are standardized \n",
    "    and normalized.   \n",
    "    ---------------------------------------------\n",
    "    Inputs: input_df: Pandas df shape (total_plays, n_features)\n",
    "            continous_feats:(ls) List of continous features\n",
    "            num_feats:(int) Number of features to select\n",
    "            prior_selected_feats: np array, features selected from training \n",
    "            feat_select: (bool) whether to select features or use previously selected\n",
    "    Returns: X: np array shape (num_samples, selected_features)\n",
    "             y: np array shape (num_samples)\n",
    "    \"\"\"\n",
    "    # scale the continous features of the input\n",
    "    scaler = StandardScaler()\n",
    "    normalizer = Normalizer()\n",
    "\n",
    "    input_df.loc[:,continous_feats] = scaler.fit_transform(input_df.loc[:,continous_feats])\n",
    "    input_df.loc[:,continous_feats] = normalizer.fit_transform(input_df.loc[:,continous_feats])\n",
    "\n",
    "    # select features if specified \n",
    "    if feat_select:\n",
    "        selector = SelectKBest(score_func = partial(mutual_info_classif, random_state=19), k = num_feats)\n",
    "        selector.fit(input_df.iloc[:,4:].drop(['pass'], axis = 1), input_df.loc[:,'pass'])\n",
    "        input_df_sel = pd.concat([input_df.iloc[:,0:4], input_df.loc[:,'pass'], input_df.loc[:,selector.get_feature_names_out()]], axis = 1)\n",
    "        \n",
    "        # save selected features\n",
    "        selected_feats = selector.get_feature_names_out()\n",
    "\n",
    "    else:\n",
    "        input_df_sel = pd.concat([input_df.iloc[:,0:4], input_df.loc[:,'pass'], input_df.loc[:,prior_selected_feats]], axis = 1)\n",
    "\n",
    "    # convert to numpy feature and response arrays \n",
    "    X = input_df_sel.iloc[:,5:].to_numpy()\n",
    "    y = input_df_sel.iloc[:,4].to_numpy()\n",
    "    \n",
    "    if feat_select:\n",
    "        return np.asarray(X).astype(np.float32), np.asarray(y).astype(np.float32), selected_feats\n",
    "    else:\n",
    "        return np.asarray(X).astype(np.float32), np.asarray(y).astype(np.float32)\n",
    "# ********************************************************************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get specifications and Load Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open json file\n",
    "with open('/Users/joe/documents/Masters_Project/NFL-Play-Call-Prediction-with-LSTM-Neural-Networks/src/specifications.json') as f:\n",
    "    specifications = json.load(f)\n",
    "\n",
    "# get specs\n",
    "data_dir, results_dir, cont_feats, max_lag = specify(specifications)\n",
    "\n",
    "# load data\n",
    "pbp = add_lagged_play_calls(pd.read_csv(data_dir), max_lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data and Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and final test sets\n",
    "train_df = pbp.iloc[0:21434,:]      # weeks 1-12\n",
    "test_df = pbp.iloc[21435:,:]        # weeks 13-17 \n",
    "\n",
    "# process data \n",
    "X_train, y_train, sel_feats = process_data(train_df, cont_feats, 60, feat_select = True)\n",
    "X_test, y_test = process_data(test_df, cont_feats, 60, prior_selected_feats = sel_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Cross Validation Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [    0     1     2 ... 14521 14522 14523] TEST: [14524 14525 14526 ... 21431 21432 21433]\n"
     ]
    }
   ],
   "source": [
    "# training and validation indices\n",
    "train_ind = np.full((14524,), -1, dtype = int)       # weeks 1-8\n",
    "val_ind = np.full((6910,), 0, dtype = int)           # weeks 9-12\n",
    "\n",
    "# val fold for PredefinedSplit\n",
    "val_fold = np.append(train_ind, val_ind)\n",
    "\n",
    "# split into training and validation\n",
    "ps = PredefinedSplit(val_fold)\n",
    "\n",
    "for train_index, test_index in ps.split():\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Custom Function for Tuning Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_baseline(hp):\n",
    "    \"\"\"\n",
    "    Chooses and builds baseline models (GBDT or Log Regr)\n",
    "    for hyperparameter tuning\n",
    "    --------------------------------------\n",
    "    Input: hp (null) A null argument that defines the\n",
    "        hyperparameter space\n",
    "        Returns: A baseline classifier\n",
    "    \"\"\"\n",
    "    # tune the type of model\n",
    "    model_type = hp.Choice(\"model_type\", [\"GBDT\", \"LOG_REG\"])\n",
    "\n",
    "    if model_type == \"GBDT\":\n",
    "        with hp.conditional_scope(\"model_type\", [\"GBDT\"]):\n",
    "            model = GradientBoostingClassifier(\n",
    "                loss = \"exponential\",\n",
    "                n_estimators = hp.Int(\"n_estimators\", min_value = 100, max_value = 500),\n",
    "                subsample = hp.Float(\"subsample\", min_value = 0.5, max_value = 1.0),\n",
    "                min_samples_split = hp.Int(\"min_samples_split\", min_value = 2, max_value = 15),\n",
    "                min_samples_leaf = hp.Int(\"min_samples_leaf\", min_value = 1, max_value = 10),\n",
    "                max_depth = hp.Int(\"max_depth\", min_value = 1, max_value = 5),\n",
    "                max_features = 'sqrt'\n",
    "            )\n",
    "    \n",
    "    else:\n",
    "        with hp.conditional_scope(\"model_type\", [\"LOG_REG\"]):\n",
    "            model = LogisticRegression(\n",
    "                penalty = \"elasticnet\",\n",
    "                l1_ratio = hp.Float(\"l1_ratio\", min_value = 0, max_value = 1),\n",
    "                solver = \"saga\"\n",
    "            )\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build tuner \n",
    "tuner = kt.tuners.SklearnTuner(\n",
    "        oracle = kt.oracles.RandomSearchOracle(\n",
    "        objective = kt.Objective('score', 'max'),\n",
    "        max_trials = 100),\n",
    "        hypermodel = build_baseline,\n",
    "        scoring = metrics.make_scorer(metrics.accuracy_score),\n",
    "        metrics = [metrics.roc_auc_score, metrics.recall_score, metrics.precision_score, metrics.log_loss],\n",
    "        cv = ps,\n",
    "        directory = results_dir,\n",
    "        overwrite = True,\n",
    "        project_name = 'baseline models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 100 Complete [00h 00m 00s]\n",
      "score: 0.7219971056439942\n",
      "\n",
      "Best score So Far: 0.7319826338639652\n",
      "Total elapsed time: 00h 04m 16s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain the Model Using Best Hyperparameters and Evaluate on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Calculate various classification metrics for a \n",
    "    baseline model, save results to dataframe\n",
    "    ----------------------------------------------\n",
    "    inputs: model: trained model object\n",
    "            X_test: test feature matrix\n",
    "            y_test: observed target values \n",
    "    output: pandas df\n",
    "    \"\"\"\n",
    "    # get the model type \n",
    "    typ = type(model).__name__\n",
    "\n",
    "    # make predictions with model \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # score predictions\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "    pre = metrics.precision_score(y_test, y_pred)\n",
    "    rec = metrics.recall_score(y_test, y_pred)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "    # save to data frame\n",
    "    return pd.DataFrame(np.array([typ,auc,pre,rec,acc,f1]).reshape(-1, 6),columns = ['Model','AUC', 'PRE', 'REC', 'ACC', 'F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store best params\n",
    "param_results = pd.DataFrame()\n",
    "\n",
    "# get 10 best parameters from search \n",
    "best_hps = tuner.get_best_hyperparameters(60)\n",
    "\n",
    "# evaluate each model on test set and save results\n",
    "for hp in best_hps:\n",
    "    # build model with hyperparams\n",
    "    mod = build_baseline(hp)\n",
    "\n",
    "    # fit model \n",
    "    mod.fit(X_train, y_train)\n",
    "\n",
    "    # evaluate\n",
    "    temp_df = evaluate_model(mod, X_test, y_test)\n",
    "\n",
    "    # add to results \n",
    "    param_results = pd.concat([param_results, temp_df], axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_results.to_csv(results_dir + '/baseline_param_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in /Users/joe/documents/mas_proj_results/baseline models\n",
      "Showing 10 best trials\n",
      "Objective(name='score', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "model_type: GBDT\n",
      "n_estimators: 408\n",
      "subsample: 0.898362662734536\n",
      "min_samples_split: 9\n",
      "min_samples_leaf: 10\n",
      "max_depth: 2\n",
      "Score: 0.7319826338639652\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "model_type: GBDT\n",
      "n_estimators: 114\n",
      "subsample: 0.998048484928794\n",
      "min_samples_split: 9\n",
      "min_samples_leaf: 6\n",
      "max_depth: 4\n",
      "Score: 0.7318379160636759\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "model_type: GBDT\n",
      "n_estimators: 114\n",
      "subsample: 0.8285771468563583\n",
      "min_samples_split: 6\n",
      "min_samples_leaf: 3\n",
      "max_depth: 4\n",
      "Score: 0.7308248914616498\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "model_type: GBDT\n",
      "n_estimators: 499\n",
      "subsample: 0.6051937252215472\n",
      "min_samples_split: 4\n",
      "min_samples_leaf: 4\n",
      "max_depth: 2\n",
      "Score: 0.7306801736613604\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "model_type: GBDT\n",
      "n_estimators: 311\n",
      "subsample: 0.9180318268779729\n",
      "min_samples_split: 14\n",
      "min_samples_leaf: 4\n",
      "max_depth: 3\n",
      "Score: 0.7295224312590448\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "model_type: GBDT\n",
      "n_estimators: 331\n",
      "subsample: 0.857733834062869\n",
      "min_samples_split: 12\n",
      "min_samples_leaf: 4\n",
      "max_depth: 2\n",
      "Score: 0.7295224312590448\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "model_type: GBDT\n",
      "n_estimators: 414\n",
      "subsample: 0.8297401174808076\n",
      "min_samples_split: 10\n",
      "min_samples_leaf: 8\n",
      "max_depth: 3\n",
      "Score: 0.7287988422575977\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "model_type: GBDT\n",
      "n_estimators: 470\n",
      "subsample: 0.6870827961764577\n",
      "min_samples_split: 13\n",
      "min_samples_leaf: 9\n",
      "max_depth: 2\n",
      "Score: 0.7283646888567293\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "model_type: GBDT\n",
      "n_estimators: 497\n",
      "subsample: 0.6554383172166525\n",
      "min_samples_split: 13\n",
      "min_samples_leaf: 4\n",
      "max_depth: 2\n",
      "Score: 0.7283646888567293\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "model_type: GBDT\n",
      "n_estimators: 228\n",
      "subsample: 0.7550303557587068\n",
      "min_samples_split: 4\n",
      "min_samples_leaf: 6\n",
      "max_depth: 2\n",
      "Score: 0.7280752532561505\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "529634df1a2b41cfed3e8f852445cc42ff69443148c5a9ce9b231d678f86b266"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tf-metal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
