{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Hyperparameters of RNN and LSTM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine an optimal model type and hyperparameters for predicting NFL play calls. A hyperband search strategy is used to tune the kind of recurrent network (simple RNN or LSTM) along with a range of hyperparameters. This script is meant to run with GPU using parallel strategies on an HPC system.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "import keras_tuner as kt\n",
    "from keras import metrics\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check TensorFlow Version and Find Available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version 2.8.0\n",
      "Number of GPUs Available:  0\n",
      "---- GPUs ----\n"
     ]
    }
   ],
   "source": [
    "print(f\"TensorFlow Version {tf.__version__}\")\n",
    "print(\"Number of GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"---- GPUs ----\")\n",
    "for gpu in tf.config.list_physical_devices('GPU'): print(gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions For Model Specs and Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specify(specs):\n",
    "    \"\"\"\n",
    "    Make specifications for running the model tuning script.\n",
    "    Sets directories for storing results and global variables.\n",
    "    ---------------------------------------------\n",
    "    Inputs: A .json file containing user specifications\n",
    "    Returns: Various directories, a list of continous features, max play lags,\n",
    "    and distribution strategy\n",
    "    \"\"\"\n",
    "    # determine whether to run locally or on hpc\n",
    "    HPC = specs['HPC']['value']\n",
    "\n",
    "    # based on hpc decision, set directories for data and storing results\n",
    "    # also decide distribution strategy for keras tuner parallelization \n",
    "    if HPC:\n",
    "        # hpc data dir\n",
    "        data_dir = os.getcwd() + '/processed_pbp.csv'\n",
    "\n",
    "        # store results \n",
    "        results_dir = os.getcwd() + '/search_results'\n",
    "        tb_dir = os.getcwd() + '/tb'\n",
    "\n",
    "        # use mirrored strategy: one device with (potentially) multiple GPUs\n",
    "        strat = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "    else:\n",
    "        # local data dir\n",
    "        data_dir = specs['LOCAL_DATA_DIR']['value']\n",
    "\n",
    "        results_dir = specs['LOCAL_RESULTS_DIR']['value']\n",
    "        tb_dir = specs['LOCAL_TB_DIR']['value']\n",
    "\n",
    "        # use one device strategy\n",
    "        strat = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "\n",
    "    # get a list of continous feature variables\n",
    "    cont_feats = specs['CONT_FEATS']['value']\n",
    "\n",
    "    # get max play lags\n",
    "    max_lag = specs['MAX_PLAY_LAG']['value']\n",
    "\n",
    "    return data_dir, results_dir, tb_dir, cont_feats, strat, max_lag\n",
    "# ********************************************************************\n",
    "def add_lagged_play_calls(input_df, max_lag):\n",
    "    \"\"\"\n",
    "    Given a pbp data frame, add feature columns\n",
    "    for up to max_lag lagged play call values. The \n",
    "    first max_lag play calls will be dropped for each \n",
    "    team.\n",
    "    ---------------------------------------------\n",
    "    Inputs: input_df: Pandas df shape (total_plays, n_features)\n",
    "            max_lag: (int) The maximum number of lagged play call\n",
    "            values to add as columns to the pbp data frame\n",
    "    Returns: A pbp data frame \n",
    "    \"\"\"\n",
    "    for lag in range(max_lag):\n",
    "        input_df['play_lag' + str(lag + 1)] = input_df.groupby(['posteam'])['pass'].shift(lag + 1)\n",
    "        \n",
    "    return input_df.dropna()\n",
    "# ********************************************************************\n",
    "def arange_by_pos_team(input_df):\n",
    "    \"\"\"\n",
    "    Given a pbp data frame, convert to a list\n",
    "    of arrays where each array is the sequence of \n",
    "    plays for each team for the full season\n",
    "    ---------------------------------------------\n",
    "    Input: Pandas df shape (total_plays, n_features)\n",
    "    Returns: List of arrays\n",
    "    \"\"\"\n",
    "\n",
    "    # get an array of the team names \n",
    "    teams = input_df['posteam'].unique()\n",
    "\n",
    "    # initialize a list to store arrays\n",
    "    teams_ls = []\n",
    "\n",
    "    # add the sequence of plays for each team to the list\n",
    "    for i in range(len(teams)):\n",
    "        pos_team = np.array(input_df[input_df['posteam'] == teams[i]])\n",
    "        teams_ls.append(pos_team)\n",
    "    \n",
    "    return teams_ls\n",
    "# ********************************************************************\n",
    "def sequence_padding(ls):\n",
    "    \"\"\"\n",
    "    Given a list of play sequence arrays, pad the \n",
    "    arrays to uniform length and combine  \n",
    "    ---------------------------------------------\n",
    "    Input: List of arrays\n",
    "    Returns: Numpy array shape (n_teams, max_play_sequence, n_features)\n",
    "    \"\"\"\n",
    "\n",
    "    # find the maximum size of the dimensions in the sequence arrays\n",
    "    max_shape = [0,0]\n",
    "    for a in ls:\n",
    "        if max_shape[0] < a.shape[0]:\n",
    "            max_shape[0] = a.shape[0]\n",
    "        if max_shape[1] < a.shape[1]:\n",
    "            max_shape[1] = a.shape[1]\n",
    "\n",
    "    # append the padded sequences and combine to 3D array \n",
    "    play_seqs = []\n",
    "    for a in ls:\n",
    "        play_seqs.append(np.pad(a, pad_width=((0, max_shape[0] - a.shape[0]),\n",
    "                                 (0, max_shape[1] - a.shape[1])), mode='constant'))\n",
    "    \n",
    "    play_seqs = np.stack(play_seqs)\n",
    "\n",
    "    return play_seqs\n",
    "# ********************************************************************\n",
    "def process_data(input_df, seq_len, feat_list):\n",
    "    \"\"\"\n",
    "    Converts a pbp data frame into X and y matrices \n",
    "    for training/validation. Continous features are standardized \n",
    "    and normalized.   \n",
    "    ---------------------------------------------\n",
    "    Inputs: input_df: Pandas df shape (total_plays, n_features)\n",
    "            seq_len: (int) The number of prior plays to condsider\n",
    "                     for modeling. E.g. a value of 10 means the model \n",
    "                     will look at a sequence of 10 plays and predict \n",
    "                     the 10th play\n",
    "\n",
    "    Returns: X: np array shape ((max_play_sequence - discarded time steps) * n_samples, seq_len, n_features)\n",
    "                The feature matrix\n",
    "             y: np array shape ((max_play_sequence - discarded time steps) * n_samples, 1, 1)\n",
    "    \"\"\"\n",
    "    # scale the continous features of the input\n",
    "    scaler = StandardScaler()\n",
    "    normalizer = Normalizer()\n",
    "\n",
    "    input_df.loc[:,feat_list] = scaler.fit_transform(input_df.loc[:,feat_list])\n",
    "    input_df.loc[:,feat_list] = normalizer.fit_transform(input_df.loc[:,feat_list])\n",
    "\n",
    "    # get the entire sequence of plays for each team\n",
    "    seqs = sequence_padding(arange_by_pos_team(input_df))\n",
    "    \n",
    "    # convert sequences to length 'seq_len'\n",
    "    flag = 0\n",
    "    for sample in range(seqs.shape[0]):\n",
    "        temp = np.array([seqs[sample,i:i+seq_len,:] for i in range(seqs.shape[1] - seq_len + 1)])\n",
    "        if flag==0:\n",
    "            X = temp\n",
    "            flag = 1\n",
    "        else:\n",
    "            X = np.concatenate((X,temp)) # feature matrix\n",
    "    \n",
    "    # output matrix (the last observation of each sequence)\n",
    "    y = X[:, seq_len-1, 24][:,np.newaxis,np.newaxis]\n",
    "\n",
    "    # remove unwanted columns\n",
    "    X = np.delete(X, 24, 2)\n",
    "    X = X[:, :, 4:]\n",
    "\n",
    "    return np.asarray(X).astype(np.float32), np.asarray(y).astype(np.float32)\n",
    "# ********************************************************************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Specifications and Load Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open json file\n",
    "with open('/Users/joe/documents/Masters_Project/NFL-Play-Call-Prediction-with-LSTM-Neural-Networks/src/specifications.json') as f:\n",
    "    specifications = json.load(f)\n",
    "\n",
    "# get specs\n",
    "data_dir, results_dir, tb_dir, cont_feats, strat, max_lag = specify(specifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "pbp = add_lagged_play_calls(pd.read_csv(data_dir), max_lag)\n",
    "\n",
    "# split into training, validation, retraining, and test sets\n",
    "train_df = pbp.iloc[0:14557,:]       # first 8 weeks of the season\n",
    "val_df = pbp.iloc[14557:21467,:]     # weeks 9-12\n",
    "retrain_df = pbp.iloc[0:21467,:]     # weeks 1-12 \n",
    "test_df = pbp.iloc[21467:,:]         # weeks 13-17    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Custom Classes for RNN/LSTM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom classes are required to tune the data preprocessing i.e. how many prior plays to consider in a sequence for the recurrent networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn class\n",
    "class rnn_hypermodel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        \"\"\"\n",
    "        Builds and compiles a recurrent neural network\n",
    "        with specified hyperparameters to tune. Chooses\n",
    "        between a simple RNN and an LSTM model. \n",
    "        ---------------------------------------------\n",
    "        Input: hp (null) A null argument that defines the\n",
    "        hyperparameter space\n",
    "        Returns: A compiled recurrent neural network\n",
    "        \"\"\"\n",
    "\n",
    "        # start with a sequential model\n",
    "        clf = keras.Sequential()\n",
    "\n",
    "        # tune the sequence length\n",
    "        seq_len = hp.Int(\"seq_len\", min_value = 1, max_value = 15)\n",
    "\n",
    "        # get the size of the feature space\n",
    "        num_feats = (92 + seq_len) - 5 \n",
    "\n",
    "        # tune which kind of recurrent model to use (simple rnn or lstm)\n",
    "        model_type = hp.Choice(\"model_type\", [\"rnn\", \"lstm\"])\n",
    "\n",
    "        if model_type == \"rnn\":\n",
    "            # add the recurrent layer; tune units, activation function, and dropout rates\n",
    "            with hp.conditional_scope(\"model_type\", [\"rnn\"]):\n",
    "                clf.add(keras.layers.SimpleRNN(units = hp.Int(\"RNNunits\", min_value = 50, max_value = num_feats),\n",
    "                                               input_shape = (seq_len, num_feats), \n",
    "                                               activation = hp.Choice(\"RNNactivation\", [\"tanh\", \"elu\"]),\n",
    "                                               dropout = hp.Float(\"RNNdropout\", min_value = 0.1, max_value = 0.5),\n",
    "                                               recurrent_dropout = hp.Float(\"RNNrec_dropout\", min_value = 0.1, max_value = 0.5)))\n",
    "\n",
    "        if model_type == \"lstm\":\n",
    "            # add the lstm layer; certain defaults are required to run GPU implementation (activations, rec dropout, unroll, use bias)\n",
    "            with hp.conditional_scope(\"model_type\", [\"lstm\"]):\n",
    "                clf.add(keras.layers.LSTM(units = hp.Int(\"LSTMunits\", min_value = 50, max_value = num_feats),\n",
    "                                          input_shape = (seq_len, num_feats),\n",
    "                                          dropout = hp.Float(\"LSTMdropout\", min_value = 0.1, max_value = 0.5)))\n",
    "    \n",
    "        # tune the number of hidden dense layers\n",
    "        for i in range(hp.Int(\"num_dense_layers\", min_value = 1, max_value = 3)):\n",
    "\n",
    "            # add dropout, tune the rate\n",
    "            clf.add(keras.layers.Dropout(rate = hp.Float(\"MLPdropout\", min_value = 0.1, max_value = 0.5)))\n",
    "\n",
    "            if i == 0:\n",
    "                # tune size of first dense layer\n",
    "                clf.add(keras.layers.Dense(units = hp.Int(\"MLPunits\" + str(i + 1), min_value = 25, max_value = 102),\n",
    "                                          activation = \"elu\",\n",
    "                                          kernel_initializer = \"he_normal\"))\n",
    "\n",
    "            if i == 1:\n",
    "                # tune size of second dense layer\n",
    "                clf.add(keras.layers.Dense(units = hp.Int(\"MLPunits\" + str(i + 1), min_value = 15, max_value = 80),\n",
    "                                          activation = \"elu\",\n",
    "                                          kernel_initializer = \"he_normal\"))\n",
    "\n",
    "            if i == 2:\n",
    "                # tune size of second dense layer\n",
    "                clf.add(keras.layers.Dense(units = hp.Int(\"MLPunits\" + str(i + 1), min_value = 5, max_value = 60),\n",
    "                                          activation = \"elu\",\n",
    "                                          kernel_initializer = \"he_normal\"))\n",
    "    \n",
    "        # add the output layer\n",
    "        clf.add(keras.layers.Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "        # tune the learning rate\n",
    "        lr = hp.Float(\"lr\", min_value=1e-5, max_value = 1e-2, sampling = \"log\")\n",
    "\n",
    "        # compile model\n",
    "        clf.compile(loss = \"binary_crossentropy\", optimizer = tf.keras.optimizers.Nadam(learning_rate=lr, clipnorm = 1.0),\n",
    "                    metrics = [metrics.AUC(), metrics.Precision(), metrics.Recall()])\n",
    "    \n",
    "        return clf\n",
    "\n",
    "\n",
    "    def fit(self, hp, model, train_df, feat_list, max_lag, validation_data = None, **kwargs):\n",
    "        \"\"\"\n",
    "        Given a model and trial hyperparams, fit to \n",
    "        training data and evaluate on validation, if provided  \n",
    "        ---------------------------------------------\n",
    "        Inputs: hp: (null) A null argument that defines the\n",
    "                hyperparameter space\n",
    "                model: (object) a compiled neural net\n",
    "                train_df: (Pandas df) input data \n",
    "                feat_list: (ls) continous features\n",
    "                max_lag: (int) maximum play call lag to consider  \n",
    "        Returns: History object\n",
    "        \"\"\"\n",
    "        \n",
    "        # get the number of lags used in model building to process data \n",
    "        seq_len = hp.get(\"seq_len\")\n",
    "\n",
    "        # get the number of lag features to remove for shape compatability\n",
    "        feats_rem = max_lag - seq_len\n",
    "        \n",
    "        # crop training df\n",
    "        cropped_train_df = train_df.iloc[:,:-feats_rem]\n",
    "\n",
    "        # get input and output training matrices\n",
    "        X_train, y_train = process_data(cropped_train_df, seq_len, feat_list)\n",
    "        \n",
    "        if validation_data is not None:\n",
    "            # crop validation data\n",
    "            cropped_val_df = validation_data.iloc[:,:-feats_rem]\n",
    "\n",
    "            # get input and output validation matrices\n",
    "            X_val, y_val = process_data(cropped_val_df, seq_len, feat_list)\n",
    "            validation_data = (X_val, y_val)\n",
    "\n",
    "        return model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            validation_data = validation_data,\n",
    "            **kwargs,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project /Users/joe/documents/mas_proj_results/testing1/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from /Users/joe/documents/mas_proj_results/testing1/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# build keras tuner for rnn model\n",
    "with strat.scope():\n",
    "    tuner = kt.Hyperband(\n",
    "        rnn_hypermodel(),\n",
    "        objective = kt.Objective(\"val_loss\", direction = \"min\"),\n",
    "        max_epochs = 80,\n",
    "        factor = 2,\n",
    "        directory = results_dir,\n",
    "        project_name = 'HPC_search1',\n",
    "        distribution_strategy = strat,\n",
    "        overwrite = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks \n",
    "lr_sched = tf.keras.callbacks.ReduceLROnPlateau(factor = 0.5)\n",
    "erly_stp = tf.keras.callbacks.EarlyStopping(patience = 10, restore_best_weights = True)\n",
    "tb = tf.keras.callbacks.TensorBoard(log_dir = tb_dir)\n",
    "\n",
    "tuner.search(\n",
    "    train_df = train_df,\n",
    "    feat_list = cont_feats,\n",
    "    max_lag = max_lag,\n",
    "    validation_data = val_df,\n",
    "    epochs = 40,\n",
    "    batch_size = 32,\n",
    "    callbacks = [lr_sched, erly_stp, tb]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain the Model Using the Best Hyperparameters and Evaluate on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best 20 models from the search\n",
    "best_hps = tuner.get_best_hyperparameters(5)\n",
    "\n",
    "# get hyper model\n",
    "hyp_mod = rnn_hypermodel()\n",
    "\n",
    "# initialize param results\n",
    "param_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hp in best_hps:\n",
    "    # build model with hyper params\n",
    "    mod = hyp_mod.build(hp)\n",
    "\n",
    "    # fit model to retrain data and evaluate on test\n",
    "    hist = hyp_mod.fit(hp, mod, retrain_df, cont_feats, max_lag, validation_data = test_df, verbose = 0)\n",
    "    \n",
    "    # join hyperparam values with metrics for this trial\n",
    "    temp_df = pd.concat([pd.DataFrame(hp.values, index = [0]), pd.DataFrame(hist.history)], axis = 1)\n",
    "    \n",
    "    # remove numbers at end of metrics for columns compatibility \n",
    "    temp_df.columns = temp_df.columns.str.replace('_[0-9]','', regex=True)\n",
    "    temp_df.columns = temp_df.columns.str.replace('[0]','', regex=True)\n",
    "\n",
    "    # join along rows with new trials \n",
    "    param_results = pd.concat([param_results, temp_df], axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_results.to_csv(results_dir + '/param_results.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
